---
title: "optimal smoothing"
date: 2017-12-10 18:47:12
slug: 
draft: true
categories: [Miscellaneous]
math: true
---

Primal Adjoint - Reverse-mode intermediate values [x]()(1). Inaccurately often called the derivative/gradient. Tangent - Forward-mode intermediate values x^(1). Often called "derivative".

The function is often referred to as the "model" (roots in simulation). So we talk of the "primal model"

Physicists really like thinking about Jacobian matrix (generalization of gradient, generalization of derivative)

So these all "mean the same thing": J ∇/D d/dx. Get used to seeing ∇ it shows up a lot.

Easy unary case

Suppose \$y = f(x)\$

Then \$y^{(1)} = \nabla f(x) x^{(1)}\$ (tangent)

And \$x\_{(1)} = \nabla f(x)^T [y](){(1)}\$ (adjoint)

What is with the transpose? To understand we have to do a little dimensional analysis:

    ∇f = df/dx0 df/dx1 df/dx2 ...

And if f has extra outputs, this looks more like:

    df0/dx0 df0/dx1 df0/dx2 
    df1/dx0 df1/dx1 df1/dx2
    df2/dx0 df2/dx1 df2/dx2

and now let us think about what is going on with forward and reverse mode,

With forward (tangent), we are able to compute df/dx0 (aka df0/dx0 df1/dx0 ...) Let us see what happens when we seed it this way:

    [ df0/dx0 df0/dx1 df0/dx2 ]    [ 1 ]     [ df0/dx0 ]
    [ df1/dx0 df1/dx1 df1/dx2 ]    [ 0 ]   = [ df1/dx0 ]
    [ df2/dx0 df2/dx1 df2/dx2 ]    [ 0 ]     [ df2/dx0 ]

With reverse (adjoint) we are able to compute df0/dx. Clearly we need to transpose for this case. For a scalar-valued function the Jacobian is just:

    df/dx0 df/dx1 df/dx2

But we don't want to compute a dot product. This is why the seed is 1 for scalar valued:

    df/dx0            df/dx0
    df/dx1    1    =  df/dx1
    df/dx2            df/dx2

OK let's pause

------------------------------------------------------------------------

So what is our setting. The setting is smoothing. Suppose that we have some sort of nondifferentiable problem \$f(\theta)\$. To differentiate it, we want to introduce some sort of smoothing factor. However, the smoothing may *bias* our estimator or increase its variance, so we may parametrize this smoothing factor as the hyperparameter p: \$f(\theta, p)\$. And now the problem is, how do we select a smoothing factor? <https://openreview.net/pdf?id=r1BWg3gAW>

We could just run a hyperparameter search, but actually we can do even better: we can optimize for p as well! The idea is that, the optimal smoothing minimizes the error of the smoothed gradient estimator. There are some details for how to do this (you can't actually directly compute the error, as that expression includes the very quantity we're trying to estimate), but we need to do an optimization over an expression that includes a gradient computation (the gradient computation is the gradient estimator!) Double-backwards time!

Let's abstract over the details some more. We have \$f(\theta, p)\$. We can compute the derivative f by reverse-mode AD, giving us a function \$(\theta\_{(1)}, [p](){(1)}) = f_d(\theta, p, [f](){(1)})\$ (remember that if we just cared about the gradient, we'd set \$f\_{(1)} = 1\$). From here, suppose we have some other function \$g(\theta\_{(1)})\$ (remember, we don't really care about the smoothing parameter p from an optimization perspective; just a hyperoptimization perspective.) To optimize the value of this function, we need to optimize the composition of these two functions.

So, intuitively, the way to do something like this:

    theta = Variable(...)
    p = Variable(...)
    y = f(theta, p)
    # TODO THIS MIGHT NEED ADJUSTING
    (grad_theta, unused_grad_p) = backward(loss(y)) # derivative of loss wrt theta (seeded with 1)
    hyperloss = g(y, grad_theta)
    (unused_grad_theta, grad_p) = backward(hyperloss) # derivative of hyperloss wrt p (seeded with 1)
    # SGD both the parameter and hyperparameter
    theta += lr * grad_theta
    p += lr * grad_p

Names here are a bit sloppy. We can be more precise using tangent/adjoint naming:

- Let us refer to loss as f, and hyperloss as g
- Seed of grad(loss,theta) = \$f\_{(1)}\$
- grad_theta = \$\theta\_{(1)}, [p](){(1)}\$
- hyperloss = \$g\$
- Seed of grad(hyperloss, p) = \$g\_{(1)}\$
- grad_p = \$\theta\_{(1)}, latex [p](){(1)}\$ (to be fixed: \$p\_{(1)}^{(2)}\$)

But let us think about what is going on when we differentiate this. When we did the first backward of loss, we didn't care about the derivative of p. Even though we don't care about p, we still want to reverse mode, because the output is a scalar loss. (food for thought: could we have optimized this too? Probably could stop executing derivative computation when you get to p. But if p and theta get put together at the very beginning, you're not going to be able to optimize this at all; you only save one step at the end)

When we did the second backward of hyperloss, we didn't care about the derivative of theta. But now there is an opportunity.

DIAGRAM

If we look at the computation diagram for hyperloss, what we see is that at some point, there is a *bottleneck*, through the computation of loss (scalar.) So now a forwards computation is ??? WHAT?

What we can do better is if we reverse mode only up to grad_theta, and then *forward* mode to grad_p. (Naming is sloppy for now)

------------------------------------------------------------------------

Let's do it.

Remember \$f(\theta, p)\$

If we reverse mode it, we get \$f_d(\theta, p, [f](){(1)})\$ (taking in the seed, usually 1)

If we forward mode it, we get \$f_t(\theta, p, \theta^{(1)}, p^{(1)})\$

What if we forward mode the reverse mode? We get: \$f\_{d,t}(\theta, p, [f](){(1)}, \theta^{(2)}, p^{(2)}, [f](){(1)}^{(2)})\$ (the second iteration gets a new index. And what does this give us? \$(\theta\_{(1)}^{(2)}, [p](){(1)}^{(2)})\$

NEED A DECODER RING FOR THE NOTATION

And because we only care about derivative on p, we only set \$p^{(2)} = 1\$ and all other tangents to 0. (Is this true?! I'm skeptical now...)
