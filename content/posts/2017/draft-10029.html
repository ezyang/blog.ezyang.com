---
title: "DLVM (Richard Wei)"
date: 2017-12-08 17:03:29
slug: 
draft: true
categories: [Miscellaneous]
---

<p>The below is a transcript of a talk by `Richard Wei <>`_ on `BrainWave <https://www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/>`_, at the `ML Systems Workshop <https://nips.cc/Conferences/2017/Schedule?showEvent=8774>`_ at NIPS'17.</p>

<p>----</p>

<p>One or two years ago, there's a world of deep learning frameworks and compiler technologies.  Now there's no combination of these two worlds.  Now there are deep learning compiler technologies.  XLA, TVM.</p>

<p>The core inspiration of DLVM is that NN is just programs. Typing, static analysis, control flow, these are instantly familiar to compiler developers. This is a new compiler program. We no longer want to treat NNs as dataflow graphs, they should be full fledged programs. We want type safety, AOT AD, code generation, and lightweight installation.  All of these things can be solved by compiler technologies.</p>

<p>What would DLVM system look like? Well, currently, all the existing DL compiler toolkits are like this: you have a Python frontend, and a C/C++ backend. But in DLVM system, we're going to throw away all of these.  Change Python to a safe language.  Start with a DSL. We want to represent NNs as a host language functions. Have type safety, enforce naturalness of expression. Two ways to do this: lightweight modular staging; a DSL technique; the other is compiler magic.  Today we are going to talk about LMS.</p>

<p>On top of that, we'll build libraries on top of DSLs, not alongside. We have trainer, layers and application API. On the bottom, we have a generic linear algebra IR; automatic differentiation as part of compiler, optimizations, code generation, and runtime support.</p>

<p>Let's start with DLVM, the compiler infrastructure. DLVM has the linear algebra IR. It's a framework for building DSLs; it's unlike the current frameworks. It's a framework for building DSLs. It has an automatic backpropagator, multistage optimizer, and static codegen based on LLVM.</p>

<p>DLVM starts with DLVM Core. It looks just like LLVM. We have analyses, verifiers, and transforms. IR, and backend for core. On top of that, you have DSL stack. Our DSL stack has two languages. TEL is a DSL. NNKit is an embedded DSL. We have an LLVM IR generator, runtime, and a command line toolchain; just like LLVM's binaries. And then below there's compiler infrastructure; LLVM helps us target different hardware, CPU and GPU.</p>

<p>DLVM has a high-level type system that supports fdirst class tensors. There are instructrions; element wise unary, binary, convolution, all of these are quite common in existing compiler tools for ML.  In addition, we add a general purpose instruction set. That enables us to build full fledged DSLs. Function application, copy, load and store. You can use them to build optional memory semantics.</p>

<p>The instruction set emphasizes primitive math operations and general purpose operations. No softmax and sigmoid, because they can be composed. No min, max, relu, because they can be composed by compare and select.</p>

<p>DLVM IR has full SSA form. CFG and basic blocks with arguments, so you don't have to add phi functions; a technique used by Swift IR. You have custom type definitions, so developers can enable their DSLs to have custom types. It has a modular architecture, within a module, you have multiple functions; multiple functions enable interprocedural optimizations for DSLs.  It has a textual format and an in-memory format, just like LLVM. Built in parser and verifier, so users and verifiers can write robust unit tests with LLVM unit tests.</p>

<p>The DLVM IR looks like this; module, and in module you have multiple functions, and control flow graph formed by basic blocks, just like LLVM. Here's what it look like: you declare a module, current stage and compilation, structs, and define functions. Where we can have functions with control flow formed by basic blocks.  Transformations on DLVM IR include two kinds. One is differentiation, and the other is optimization.</p>

<p>Let's take a look at differentiation. I have an inference function which computes xw+b. To differentiate this function; since we have multiple functions, we can directly make a gradient declaration, the gradient of inference with respect to arguments one and two, returns a tuple that's the gradient with respect to those two arguments. When you run differentiation, it canonicalizes each declaration into a full function definition. When it runs, it creates a basic block, copies instructions from forward, and generates the adjoint coem and then runs som esimplification passes, dead code elimination, giving you the gradient.  Also, gradient decoration is differentiable. You can start by using from, multiple outputs as a tuple, you can select which output to select differentiation, you can specify wrt, you can keep original outputs, and control seedable, passing in backpropagated gradients as seed.</p>

<p>Suppose you have two functions f and g, f computes xw+b, and g computes xwb (tutorial on how reverse mode automatic differentiation works)</p>

<p>The compilation pipeline goes through analysis. Dominance, side effects, etc. We don't differentiability for every function, but if you differentiate a function that's not differentiable, the compiler will give a compile time error. It turns stage to optimizable, because now you have a canonicalized representation, and runs a bunch of optimization, like AD checkpointing, linear algebra simplification, traditional compiler optimizations. Compute generation, compute scheduling; a lowering pass. llgen generates LLVM IR. That's the compilation pipeline.</p>

<p>Here's DLVM. Now, when we have this full fledged compiler infrastructure, we can start building DSLs on top of that. DSLs should be treating NNs as a program, not a graph.  When we have that, this is not enough. We also want to apply static analysis, and type safety, and naturalness of programming. So we present NNKit, a staged DSL in Swift. It's a prototype, represents tensor computation embedded in host language. It has type safety, generates DLVM IR on the fly. It has static/rank tensors, because we want to enhance rank safety first, type wrapper for staging (rep type, means the rep of some computation, that's guaranteed to produce a value of type, tensor 1D of float, tensor 2D of T). Operator overloading...</p>

<p>Ultimately, we want to turn the representation of some function to an actual function. The way we do that, is starting with expression staging, when we run the function, it goes through shape specialization, and then we generate DLVM IR, it goes through DLVM, and LLVM, generates binary, and then we need to get it back to swift; function reification. Now the type is Float2D -> Float2D.</p>

<p>(worked example)</p>

<p>We believe the future of ML is Swift. DLVM is written entirely in Swift. In PL and compiler world, there are lots of thing the ML community can learn from. It's programs, type safety, AOT AD and code generation.  That's DLVM, take a look at dlvm.org, and later next year we'll be open sourcing it.  We'll start by core IR, and then code generator.</p>

<p>Q: You mention full SSA, but do you even need to translate into SSA? Computation graph, the transformation is already SSA.</p>

<p>A: Yes, but to support DSLs for expressivity, you need something like SSA to support side effects. (But after that, do you have phi node) No, because we're using basic blocks with arguments. That's a combination of CPS with SSA. (ezyang: No! It's not that!)  Phi node is very cumbersome, phi node is always first in basic block; in Swift IR you don't have to verify it; it's very natural. (Can you tell me why I should use DLVM over TF?)  DLVM's intention was... motivation was never to chase the FLOPs, or to push the limit of performance.  It's original motivation was to build the right compiler infrastructure, make it scalable, ops as primitive as possible, make the right infra for writing optimization passes.</p>

<p>Q: What would you do with an operation like SoftMax, which could be optimized with some custom reductions, custom code?</p>

<p>A: That could be done by pattern matching. (So you would have to match the pattern that looks like softmax?) Yes. That's not a case I'm considering right now, but you can use pattern matching to lower it into custom computation.</p>

<p>Q: What's your approach to static versus dynamic tensor sizes? That seems to be a major point of contention. TF graph, ONNX graph representation, all of these possibilities.</p>

<p>A: Right now, DLVM supports only fully static tensor shapes, just like XLA. In the future, it may make sense to add a shape generic system, to define shape transformations. That will make the codegen more powerful.</p>

<p>Q: How's the support on Linux for Swift, IDEs?</p>

<p>A: There is no IDE support for now, but the language support is getting better and better. For Swift 3, the libraries were not fully ready, but Swift 4 it's ready. Going forward, Swift really has the right features. Easy to learn, fast, and that's the most important thing, to get the ML community on type safety and better software engineering.</p>