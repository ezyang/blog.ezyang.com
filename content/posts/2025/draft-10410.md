---
title: ""
date: 2025-01-03 13:32:06
slug: 
draft: true
categories: [Miscellaneous]
---

In my previous two posts "\`Ways to use torch.compile \<http://blog.ezyang.com/2024/11/ways-to-use-torch-compile/\>\`\_" and "\`Ways to use torch.export \<http://blog.ezyang.com/2024/12/ways-to-use-torch-export/\>\`\_", I often said that PyTorch would be good for a use case, but there might be some downsides. Some of the downsides are foundational and difficult to remove. But some... just seem like a little something is missing from PyTorch. In this post, I want to lay out a vision of the future based on fixing some of the downsides. XXX

# Improving torch.compile

**The compiler is complicated.** Recently in PyTorch HQ, we have gotten serious about defining a *programming model for PT2.* A programming model is a an abstract description of the system that is both simple (so anyone can understand it and keep it in their head all at once) and can be used to predict the system's behavior. The [torch.export programming model](https://docs-preview.pytorch.org/pytorch/pytorch/143546/export.programming_model.html) is an example of such a description. Beyond export, we would like to help users understand why all aspects of PT2 behave the way it does (e.g., via improved error messages), and give simple, predictable tools for working around problems when they arise. This is a big effort and I hope we can share more about this effort soon.

**Just in time compilation is operationally complex.** Whenever someone realizes that torch.compile compilation is consuming a substantial amount of wall time on expensive cluster machines, the first thing they ask is, "Why don't we just compile the model in advance?" Now, you can't "just" compile the model in advance; to run torch.compile, you need to have concrete values for all of your inputs (inlining!)--if your model has multiple graph breaks, you need the inputs for every graph break! The most straightforward way to collect all of these inputs is to

Compile time can be long / just in time compilation is operationally complex / caches are not guaranteed to hit / split into multiple graphs - torchnative / Pre-compilation for training

Compile time can be long - Caching (continued)

Multithreading is buggy - Fix multithreading

# Improving torch.export

Upfront cost for export - Draft mode

You need libtorch to run the model - Libtorch-free AOTInductor <https://github.com/lianakoleva/no-libtorch-compile>

Custom operators not packaged - Bundle CUDA kernels into AOTI

Guard-based dispatch - Export multigraphs

First class kernel authoring in export / ABI stable PyTorch extensions
