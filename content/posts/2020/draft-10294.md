---
title: ""
date: 2020-12-17 11:14:02
slug: 
draft: true
categories: [Miscellaneous]
---

One of the fun things about working on a deep learning library is that programming languages concepts are everywhere. Here is one recent application of bidirectional lenses to the problem of forward mode automatic differentiation in the presence of mutation.

Forward mode automatic differentiation can easily be defined using dual numbers: you represent your quantity as x plus dx, where dx is some infinitesimal perturbation that obeys normal rules, except that dx \* dy = 0. If you're a tensor library like PyTorch, x and dx are tensors, conventionally called the primal and tangent. (For now let us not worry about nested forward AD or perturbation confusion). It's pretty easy to strap on forward AD to a library by just defining a dual data structure consisting of two tensors, and then going forth and defining all relevant operations on it. It's a fun and short exercise and I recommend attempting it in your favorite programming language.

Forward AD designed in this way involves a static type separation: we distinguish between a tensor and a dual tensor. This means that the two worlds don't straightforwardly mix: you can't just do an operation between a tensor and a dual tensor, you have to lift the tensor to be a dual tensor and then do whatever operation you wanted in that case. It turns out users hate doing lifting by hand, and so you're usually stuck with figuring out with how to do lifting by hand. Which, in a system like PyTorch, means that *every tensor is secretly actually a dual tensor*, but the tangent just happens to be zero most of the time.

`` `python # In fact, make_dual is a bijective lens def make_dual_get(p: Tensor, t: Tensor) -> DualTensor:     return (p, t) def make_dual_putback(p: Tensor, t: Tensor, d: DualTensor) -> Tuple[Tensor, Tensor]:     return (d.primal, d.tangent) def primal_get(d: DualTensor) -> Tensor     return d.primal def primal_putback(d: DualTensor, p: Tensor) -> DualTensor     return (p, d.tangent)      # Proceeds similarly for tangent ``<span class="title-ref"> However, we may also wish to say that there is only one concept of a tensor in our system; every tensor is a dual tensor (and simply non-dual tensors are those for whom the tangent is zero.) In that case, we need a slightly more complex semantics when defining lifted versions of all our operations: </span>`python @dataclass class DualTensor:     primal: Tensor     tangent: Optional[Tensor] def make_dual_get(p: DualTensor, t: DualTensor) -> DualTensor:     # TODO: I'm not sure if these asserts are necessary     assert p.tangent is None     # I believe this is the semantics implied by     # https://colab.research.google.com/drive/1hXB5g1ouHXHFYf9pDTzH6pG2q0Wyz92V?usp=sharing     # when there is only one level of nesting, but I'm not sure     assert t.tangent is None     return DualTensor(p.primal, t.primal) def make_dual_putback(p: DualTensor, t: DualTensor, d: DualTensor) -> Tuple[DualTensor, DualTensor]:     d_tangent = d.tangent     if d_tangent is None:         d_tangent = torch.zeros_like(d.primal)     # Preserve p.tangent and t.tangent!     return (DualTensor(d.primal, p.tangent), DualTensor(d_tangent, t.tangent)) def primal_get(d: DualTensor) -> DualTensor     return DualTensor(d.primal, None) def primal_putback(d: DualTensor, p: DualTensor) -> DualTensor     return DualTensor(p.primal, d.tangent)  # Preserve d.tangent! # Proceeds similarly for tangent`<span class="title-ref"> The most important things to observe are in the definitions of putback, we preserve the pre-existing tangents on the original dual tensors (prior to make_dual or primal). This preservation is essential for maintaining stability: if we didn’t preserve the tangent, then primal_putback(primal_get(d), d) != d (the left hand side would have lost the tangent!) \### Example: putback transfers tangents Let’s take this example from https://github.com/albanD/pytorch/pull/1/files </span>`python # dual is a dual Tensor out = torch.zeros(100) out[2] = dual`<span class="title-ref"> Intuitively, we think that out should become a dual tensor after this operation. How is this born out by the semantics? To do this, we first have to define a get and putback for indexing location two in a tensor. </span>`python def index2_get(d: DualTensor) -> DualTensor:     return DualTensor(d.primal[2], d.tangent[2]) def index2_putback(d: DualTensor, r: DualTensor) -> DualTensor     # We can't conveniently express functional putbacks in PyTorch's     # functional API, so we clone the relevant Tensors and then use     # the mutable API to perform the operations we want     p = d.primal.clone()     p[2] = r.primal     if r.tangent:         if d.tangent is None:             t = torch.zeros_like(p)         else:             t = d.tangent.clone()                 t[2] = r.tangent     else:         if d.tangent is None:             t = None         else:             t = d.tangent.clone()             t[2] = 0     return DualTensor(p, t)`<span class="title-ref"> Take a moment to verify that all of the laws are preserved. It is now easy to see that when we do a putback on a tensor, if the tensor dual (r in the function) we are putting into out (d in the function) has a non-zero tangent, this will force the new version of output to have a non-zero tangent as well. \### Example: views can have different perturbations Desmaison et al have suggested that perturbations should be associated with memory locations. This would make it effectively impossible to have two views on the same memory with different perturbations. In the semantics we have defined in this paper, this situation would not occur. Let’s consider this example: </span>`python x = torch.zeros(100) x2 = x[2] y = make_dual(x2, t_y) z = make_dual(x2, t_z) y.add_(dual)`\`

We have multiple dual operations going on in this example: we view on x, make_dual on the view, and then perform our inplace operation. Furthermore, there is another alias on x in the form of z. To understand the semantics of this program, we must apply our semantics in two steps:

- When we perform an inplace mutation on some view (e.g., y), we must use putback operations to reapply the inplace mutation all the way to the base tensor (e.g., x)
- Once we have done so, for all child views of any tensor which was viewed from the base or any of its children (e.g., z), we must use get operations to recompute what their new value would be after the modification from putback

So, in the example above, we have to apply the following steps:

1.  Compute y + dual
2.  Upstream this update to x2 using make_dual_putback
3.  Upstream this update to x using index2_putback
4.  Downstream the update to z (from x2) using make_dual_get

Let’s hand apply these operations:

`` `python y_new = y + dual x2_new, t_y_new = make_dual_putback(x[2], t_y, y_new) x_new = make_index2_putback(x, x2_new) z_new = make_dual_get(x2_new, t_z) ``\`

Notably, in the make_dual_putback, the primal and tangent of y_new are distributed into x2_new and t_y_new. This means that the tangent of x\[2\] is NOT updated; all of the tangent of dual is doing is modifying t_y! In the end, y has an updated tangent, but z is not updated at all. Although y and z alias the same primal, their tangents are not aliased, and thus inplace updates to tangent affect one but not the other.

In PyTorch, we have a thing called a view. Views let you

In PL, we're used to thinking about mutation as a thing that always makes

Bidirectional programming \[1\](<https://www.cis.upenn.edu/~bcpierce/papers/lenses-etapsslides.pdf>) \[2\](<https://www.cis.upenn.edu/~bcpierce/papers/wagner-thesis.pdf>) is a programming discipline where you have two different representations of otherwise similar information, and editing one representation causes the other representation to be updated. The classic example in the bidirectional literature is that you have a database, and you have computed a view on the database which you modify—you want to propagate this modification back to the un-viewed database.

However, in PyTorch, we have our own example of bidirectional lenses, which are *tensor views*. When we make a modification to a view of a tensor, we expect the base tensor to also get updated. Inplace updates on views are easy enough to understand in basic PyTorch, but with the addition of conjugate views and dual tensors, the more sophisticated language of bidirectional lenses can help us answer some semantic questions.

\### Bidirectional lenses in a nutshell

In the semantic universe of lenses, we are responsible for defining a get function and a putback function: \![Basic view\](RFC-0007-assets/bidirectional_lens.png)

The “get” function, in the context of tensors, is the actual view operation, whereas the putback is how we map mutations on that view back to the base tensor. Each view operation (e.g., view, squeeze, etc.) is defined as both a get and a putback function.

There are some laws that the get and putback function should obey:

- *Acceptability*: get(putback(t, s)) = t (if we put something into the source, we should get it out again if we get it out later)
- *Stability*: putback(get(s), s) = s (if the target doesn’t change, neither should the source)
- *Forgetfulness*: putback(t2, putback(t1, s)) = putback(t2, s) (each update completely overwrites the effect of the original one)

It’s easy to see that conventional inplace view updates on tensors satisfy these properties  
(in the absence of overlapping strides, anyway!)
