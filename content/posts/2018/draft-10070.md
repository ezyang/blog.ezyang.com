---
title: "Thoughts on language design for deep learning"
date: 2018-03-06 11:32:48
slug: 
draft: true
categories: [Miscellaneous]
---

How would you design a programming language for expressing PyTorch or TensorFlow style computations? What in the language should be similar to existing languages and what would be different?

**Mandatory reference counting and/or regions memory management.** Unlike traditional languages where it is not too bad to retain some dead data temporarily until the next GC occurs, prompt deallocation of tensors used in ML models is extremely important, since they are so big and you will probably run out of memory before the next GC happens. Reference counting or region-based memory management are better choices for a setting like this. The usual problem with cyclic data structures doesn't apply here, because a tensor never has pointers to other tensors.

**Purity.** You can go very far without introducing any type of computational effects, as most models are very mathematical in nature and fit the language of pure functional programming. When ML people talk about models like these, they usually call them "graphs", in the same prevailing style of static frameworks. Amazingly, the pure fragments of these languages are, for the most part, not Turing complete (the APL programming style endemic to tensor computation obviates the need for loops and primitive recursion).

But insisting on purity will get you in trouble in two major ways:

1.  A lot of algorithms ML researchers would like to write are imperative in nature, and you want these algorithms to be expressible in the direct style in your language;
2.  A pure operation from tensor to tensor requires twice as much memory as an operation on an equivalent mutable tensor. Sometimes, "twice as much" is too stiff a price to pay.

So there is also a strong motivation to want to "script" one's models, and this is exactly the niche of dynamic frameworks.

What has happened here is that we are defining not one but two languages: a pure, total (Turing-incomplete), APL-based, graph language, and an impure, Turing-complete, scripting language. But, of course, no one wants to write two languages; they want to write one language and have it all work. (If you don't mind two languages, have a chat with Haskell; she's got some monads to sell you)

**Evaluation order.** In the pure, total graph language, any evaluation order works; you are free to reorder and parallelize execution however you like. Unlike traditional programming languages, the primitive operations being executed are beefy, and all of that research you did in the nineties on fine-grained parallelism and nested data parallelism are suddenly relevant again.

In the impure scripting language, call-by-value is the order of the day. (Perhaps with a dash of asynchrony; failure is *not an option*.)

**Effects.** Deep learning models care about a much different set of computational effects than traditional programming languages. Mutable state is in, but exceptions are out (hard changes in control flow are not differentiable!) Print statements are essential for debugging, but general input/output is very simple (mostly, you are giving your prediction to another system which actually does the work.) Most of your developers won't care about defining custom effects, but you had better make your optimizer play nicely with all of the things they want to do.

Is automatic differentiation an effect? It can be implemented with delimited continuations, so maybe it is. Your users will demand it, and you'll need both a symbolic differentiator and an "autograd" tape-based automatic differentiator.

Unusually, users from the probabilistic programming community would ask you for *nondeterminism* as an effect, so that they can more easily resample their models. In this respect, they are supremely ill-served by dynamic frameworks in classic impure languages.

**Syntax.** You will pry Python from the cold, dead hands of your data scientists.

**Functions.** All of your functions will be first-order, because all of your maps and reduces will have already been handled by the APL-like operators in your language, and that's the only thing your users would have been tempted to use higher-order functions for.

**Type system.** You will have rich types (tensors with *shapes*), and you will want to infer most of them, because your users will be too lazy to write any explicit type annotations. Fortunately, your language is first-order, so basically anything will work. If you are lucky, someone will write a bidirectional type inference algorithm that can help you avoid having to annotate the shapes of your top level functions. Unfortunately, the automatically inferred equations relating the sizes of inputs and outputs will be too difficult for anyone to understand, and so you'll still have to write them manually (and hopefully machine check them for correctness).

Fortunately for you, *everything* is a tensor, including scalars (they're just zero-dim tensors.)

**Compile time.** The rise of libraries like [tensor comprehensions](https://github.com/facebookresearch/TensorComprehensions) means that compiler writers will have to take a hard look at how to design the compiler user experience when compiling your code involves an autotuning step that requires a cluster of GPU machines over several days.
