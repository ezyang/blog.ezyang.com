---
title: "Inductively defining matrices (linear functions) in Haskell"
date: 2018-09-30 10:46:43
slug: 
draft: true
categories: [Miscellaneous]
---

I spent some time this weekend trying to reimplement Conal Elliott's [The Simple Essence of Automatic Differentiation](https://arxiv.org/pdf/1804.00746.pdf) from scratch. It's not too difficult to scribble down an interpreter of category theory in the category of functions, but we put the pedal to the metal when we want to move to the category of **linear maps** (a waypoint on our way to the category of differentiable functions, the main subject of this paper). Conal has written about the category of linear maps to some degree, both in previous [papers](http://conal.net/papers/compiling-to-categories/compiling-to-categories.pdf) as well as [blog posts](http://conal.net/blog/posts/simpler-more-efficient-functional-linear-maps) (also, see [here](http://conal.net/blog/posts/reimagining-matrices)), however, I found this conceptual universe of linear maps quite unfamiliar (despite the fact that I [sling around matrices all day at work](https://github.com/pytorch/pytorch/)). In this blog post, I want to try to reexplain Conal's observation that we can represent linear maps as *free vector spaces, memoized using tries.*

First, why do we need to do all of this anyway? That is to say, why can't we just define our linear maps using matrices (any finite dimensional vector space is isomorphic to an appropriate R^n, and a linear map between two finite dimensional vector spaces can be represented using a matrix) and call it a day?

If we want to talk

A linear function is a map f between two vector spaces which preserves vector addition (`f(x + y) = f(x) + f(y)`) and scalar multiplication (`f(a*x) = a*f(x)`).
